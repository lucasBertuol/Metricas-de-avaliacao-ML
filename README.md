# M√©tricas de Avalia√ß√£o em Machine Learning
M√©tricas de avalia√ß√£o s√£o medidas quantitativas usadas para **avaliar a performance** de um modelo. Estas medidas s√£o cruciais n√£o s√≥ para avaliar o desempenho ao longo do tempo como tamb√©m para comparar diferentes modelos, otimizar hiperpar√¢metros e alinhar a avalia√ß√£o do seu algotirmo com os objetivos do seu problema espec√≠fico. As principais m√©tricas de avalia√ß√£o s√£o **Acur√°cia,  Precis√£o, Sensibilidade (Recall), Especificidade, F1 (F-Score) e AUC-ROC curve**. Mais adiante, ser√° explorado o que cada uma dessas medidas significa e como podemos obt√™-las. 
## Objetivos üéØ
- **Treinar uma rede neural** para classificar imagens em 2 classes: gatos e cachorros
- Avaliar a performance dessa rede neural calculando diferentes **m√©tricas de avalia√ß√£o**
- Analisar o **significado** desses resultados
## Estrutura do projeto üóÇÔ∏è
- **M√©tricas.ipynb:** Jupyter notebook com o c√≥digo utilizado e coment√°rios
- **PetImages:** Dataset com 23.424  imagens (11.749 gatos, 11.675 cachorros)
- **Imagens**: Pasta contendo matrizes de confus√£o e gr√°ficos de performance
## Recursos Utilizados ‚öíÔ∏è
- **Linguagem:**  Python 3.9
- **Frameworks:** TensorFlow 2.8, Keras 2.8
- **Bibliotecas:** Numpy, Matplotlib, seaborn, scikit-learn
- **Dataset:** Kaggle Cats and Dogs Dataset da Microsoft, que √© uma vers√£o mais compacta do dataset original. Dispon√≠vel nesse [link](https://www.microsoft.com/en-us/download/details.aspx?id=54765). 
## Implementa√ß√£o üßë‚Äçüíª
O nosso problema √© uma tarefa de **classifica√ß√£o bin√°ria em 2 classes**, em que "cachorro" (1) √© a nossa classe postitiva e "gato" (0) a classe negativa. 
Para que possamos medir m√©tricas de avalia√ß√£o, antes precisamos **contruir e treinar um modelo**. Entretando, eu irei me concentrar mais na etapa de **avalia√ß√£o dos resultados**, que √© o foco do projeto. 

Depois de importar as bibliotecas necess√°rias, defini o tamanho do dataset e o dividi em **treino (70%), valida√ß√£o (20%) e teste (10%)**.

A seguir eu treinei uma **rede neural** usando data augmentation e uma arquitetura robusta (8 camadas convolucionais + 2 camadas densas) para extrair features relevantes e classificar as imagens do dataset corretamente. Verifique o arquivo M√©tricas.ipynb para mais detalhes. Um resumo do modelo pode ser visto a seguir:
<p align="center" width="100%">
</p>
Ap√≥s treinar por 30 epochs com Learning rate adaptativa obtive resultados satisfat√≥rios na valida√ß√£o e salvei os melhores pesos. Ent√£o, treinei o modelo no dataset de teste, onde o modelo exibiu esses indicadores:
<p align="center" width="100%">
</p>

## Avalia√ß√£o de resultados üìà
O primeiro passo para gerar as m√©tricas de avalia√ß√£o √© calcular a **Matriz de Confus√£o**. Se trata de uma tabela que compara os valores previstos (nas colunas) com os valores reais (nas linhas) de um conjunto de dados. Desse modo, teremos quatro tipos de valores:
 - **VP** (Verdadeiros Positivos): n√∫mero de previs√µes positivas que realmente eram positivas
 - **FP** (Falsos Positivos): n√∫mero de previs√µes positivas que na verdade eram negativas
 - **VN** (Verdadeiros Negativos): n√∫mero de previs√µes negativas que realmente eram negativas
 - **FN** (Falsos Negativos): n√∫mero de previs√µes negativas que na verdade eram positivas
Para construir a matriz, temos que converter o nosso dataset de teste (um tensor) em arrays de imagens e r√≥tulos:
<p align="center" width="100%">
</p>
Ent√£o, iremos definir as previs√µes do nosso modelo e classific√°-las usando um **threshold padr√£o**, de >=0.5. Isso significa que se o nosso modelo tiver **50%** ou mais de certeza que um animal √© um cachorro, esse animal ser√° classificado como cachorro (1). Sen√£o, o animal ser√° classificado como gato (0). 
Para ter uma vis√£o completa da **distribui√ß√£o das classifica√ß√µes** em VP, VN, FP e FN eu criei duas matrizes de confus√£o, uma com propor√ß√µes e outra com valores absolutos:
<p align="center" width="100%">
</p>
Usando esses valores absolutos podemos calcular manualmente cada m√©trica usando suas respectivas f√≥rmulas:
- **Acur√°cia** =   (vp + vn) / (vp + vn + fp + fn) <br>
	A propor√ß√£o de previs√µes corretas do total de elementos. 	
- **Precis√£o** = vp / (vp + fp) if (vp + fp) > 0 else 0.0  
	A propor√ß√£o de previs√µes positivas verdadeiras do total de previs√µes positivas. 
- **Sensibilidade (Recall)** = vp / (vp + fn) if (vp + fn) > 0 else 0.0  
	A propor√ß√£o de previs√µes positivas verdadeiras to total de inst√¢ncias positivas. 
-  **Especificidade** = vn / (vn + fp) if (vn + fp) > 0 else 0.0
	A propor√ß√£o de previs√µes negativas verdadeiras do total de inst√¢ncias negativas. 
-  **F1 (F-Score)** = (2*vp) / (2*vp + fp + fn) if (2*vp + fp + fn) > 0 else 0.0 
	A m√©dia harm√¥nica entre Precis√£o e Recall, √∫til para datasets desbalanceados. 
*Obs: if > 0 else 0.0 evita divis√£o por zero.

Todas as m√©tricas calculadas aqui dependem do threshold espec√≠fico que definimos (>=0.5).
**Resultados:**
- **Acur√°cia:** Em 92% das imagens o modelo acerta a classe.
- **Precis√£o:** Entre todas as imagens que o modelo previu como "c√£o", 91% realmente s√£o c√£es. Se o modelo rotula "c√£o", h√° alta confian√ßa de que √© c√£o. 
- **Sensibilidade (Recall):** De todos os c√£es do dataset, o modelo identifica 92% como "c√£o". Poucos c√£es s√£o classificados err√¥neamente como gatos. 
- **Especificidade:** De todos os gatos do dataset, o modelo identifica 92% como "gato". Poucos gatos s√£o classificados err√¥neamente como c√£es. 
- **F1:** Bom equil√≠brio entre n√£o acusar gato de ser c√£o (FP) e n√£o deixar c√£es passarem despercebidos (FN). 

Por fim, para medir a habilidade do classificador de **distinguir entre as classes** vamos visualizar a curva AUC-ROC. Essa m√©trica n√£o depende de um threshold espec√≠fico, ela gera uma vis√£o geral da performance do modelo ao longo de todos os thresholds, sendo muito √∫til para quando temos classes desbalanceadas. As medidas utilizadas para calcular a curva s√£o: TPR (True Positive rate), que √© o mesmo que Recall e FPR, que representa com qual frequ√™ncia o modelo classifica incorretamente inst√¢ncias negativas como positivas. Quanto mais **pr√≥xima de 1.0** a TPR for, e **maior a √°rea sob a curva** mais preciso o modelo.
## Conclus√£o üê±üê∂
Este projeto possibilitou uma compreens√£o mais profunda sobre a **import√¢ncia** das m√©tricas de avalia√ß√£o na an√°lise de modelos de intelig√™ncia artificial. Mais do que apenas treinar uma rede neural, o foco esteve em **interpretar seus resultados** e entender como medidas como acur√°cia, precis√£o, recall, especificidade, F1 e AUC-ROC refletem diferentes aspectos da performance. Essas m√©tricas s√£o fundamentais para diagnosticar erros, comparar modelos e alinhar a escolha do classificador com os **objetivos do problema real**, tornando-se indispens√°veis no processo de desenvolvimento de solu√ß√µes robustas em machine learning.
