# M√©tricas de Avalia√ß√£o em Machine Learning
M√©tricas de avalia√ß√£o s√£o medidas quantitativas usadas para **avaliar a performance** de um modelo. Estas medidas s√£o cruciais n√£o s√≥ para avaliar o desempenho ao longo do tempo como tamb√©m para comparar diferentes modelos, otimizar hiperpar√¢metros e alinhar a avalia√ß√£o do seu algotirmo com os objetivos do seu problema espec√≠fico. As principais m√©tricas de avalia√ß√£o s√£o **Acur√°cia,  Precis√£o, Sensibilidade (Recall), Especificidade, F1 (F-Score) e AUC-ROC curve**. Mais adiante, ser√° explorado o que cada uma dessas medidas significa e como podemos obt√™-las. 
## Objetivos üéØ
- **Treinar uma rede neural** para classificar imagens em 2 classes: gatos e cachorros
- Avaliar a performance dessa rede neural calculando diferentes **m√©tricas de avalia√ß√£o**
- Analisar o **significado** desses resultados
## Estrutura do projeto üóÇÔ∏è
- **M√©tricas.ipynb:** Jupyter notebook com o c√≥digo utilizado e coment√°rios
- **PetImages:** Dataset com 23.424  imagens (11.749 gatos, 11.675 cachorros)
- **Imagens**: Pasta contendo matrizes de confus√£o e gr√°ficos de performance
## Recursos Utilizados ‚öíÔ∏è
- **Linguagem:**  Python 3.9
- **Frameworks:** TensorFlow 2.8, Keras 2.8
- **Bibliotecas:** Numpy, Matplotlib, seaborn, scikit-learn
- **Dataset:** Kaggle Cats and Dogs Dataset da Microsoft, que √© uma vers√£o mais compacta do dataset original. Dispon√≠vel nesse [link](https://www.microsoft.com/en-us/download/details.aspx?id=54765). 
## Implementa√ß√£o üßë‚Äçüíª
O nosso problema √© uma tarefa de **classifica√ß√£o bin√°ria em 2 classes**, em que "cachorro" (1) √© a nossa classe postitiva e "gato" (0) a classe negativa. 
Para que possamos medir m√©tricas de avalia√ß√£o, antes precisamos **contruir e treinar um modelo**. Entretando, eu irei me concentrar mais na etapa de **avalia√ß√£o dos resultados**, que √© o foco do projeto. 

Depois de importar as bibliotecas necess√°rias, defini o tamanho do dataset e o dividi em **treino (70%), valida√ß√£o (20%) e teste (10%)**.

A seguir eu treinei uma **rede neural** usando data augmentation e uma arquitetura robusta (8 camadas convolucionais + 2 camadas densas) para extrair features relevantes e classificar as imagens do dataset corretamente. Verifique o arquivo M√©tricas.ipynb para mais detalhes. Um resumo do modelo pode ser visto a seguir:
<p align="center" width="100%">
	<img width="492" height="81" alt="image" src="https://github.com/user-attachments/assets/2865f7d1-1500-4cc1-85d7-fd6812a92e35" /> 
</p>

Ap√≥s treinar por 30 epochs com Learning rate adaptativa obtive resultados satisfat√≥rios na valida√ß√£o e salvei os melhores pesos. Ent√£o, treinei o modelo no dataset de teste, onde o modelo exibiu esses indicadores:
<p align="center" width="100%">
	<img width="662" height="48" alt="image" src="https://github.com/user-attachments/assets/c781b030-d011-49eb-b69f-6678129543c0" />
</p>

## Avalia√ß√£o de resultados üìà
O primeiro passo para gerar as m√©tricas de avalia√ß√£o √© calcular a **Matriz de Confus√£o**. Se trata de uma tabela que compara os valores previstos (nas colunas) com os valores reais (nas linhas) de um conjunto de dados. Desse modo, teremos quatro tipos de valores:
 - **VP** (Verdadeiros Positivos): n√∫mero de previs√µes positivas que realmente eram positivas
   
 - **FP** (Falsos Positivos): n√∫mero de previs√µes positivas que na verdade eram negativas
   
 - **VN** (Verdadeiros Negativos): n√∫mero de previs√µes negativas que realmente eram negativas
   
 - **FN** (Falsos Negativos): n√∫mero de previs√µes negativas que na verdade eram positivas


Para construir a matriz, temos que converter o nosso dataset de teste (um tensor) em arrays de imagens e r√≥tulos:
<p align="center" width="100%">
	<img width="562" height="182" alt="image" src="https://github.com/user-attachments/assets/8d1cc19b-fe38-41d0-9429-060a441d7068" />
</p>

Ent√£o, iremos definir as previs√µes do nosso modelo e classific√°-las usando um **threshold padr√£o**, de >=0.5. Isso significa que se o nosso modelo tiver **50%** ou mais de certeza que um animal √© um cachorro, esse animal ser√° classificado como cachorro (1). Sen√£o, o animal ser√° classificado como gato (0). C√≥digo:
<p align="center" width="100%">
	<img width="604" height="187" alt="image" src="https://github.com/user-attachments/assets/9a52dc70-403c-4835-9c8c-6b9444547fdd" />
</p>

Para ter uma vis√£o completa da **distribui√ß√£o das classifica√ß√µes** em VP, VN, FP e FN eu criei duas matrizes de confus√£o, uma com propor√ß√µes e outra com valores absolutos:
<p align="center">
	<img width="400" height="450" alt="image" src="https://github.com/user-attachments/assets/b44be0b9-63e1-4573-8cab-a6a84c90b270" />
	<img width="400" height="450" alt="image" src="https://github.com/user-attachments/assets/7a85c75a-d2bb-4d95-8d9f-0df275a7f199" />
</p>
Usando esses valores absolutos podemos calcular manualmente cada m√©trica usando suas respectivas f√≥rmulas: 

- **Acur√°cia** =  (vp + vn) / (vp + vn + fp + fn) <br>
	A propor√ß√£o de previs√µes corretas do total de elementos.  <br><br>

- **Precis√£o** = vp / (vp + fp) if (vp + fp) > 0 else 0.0  
	A propor√ß√£o de previs√µes positivas verdadeiras do total de previs√µes positivas. <br><br>

- **Sensibilidade (Recall)** = vp / (vp + fn) if (vp + fn) > 0 else 0.0  
	A propor√ß√£o de previs√µes positivas verdadeiras do total de inst√¢ncias positivas. <br><br>

-  **Especificidade** = vn / (vn + fp) if (vn + fp) > 0 else 0.0 <br>
	A propor√ß√£o de previs√µes negativas verdadeiras do total de inst√¢ncias negativas. <br><br>

-  **F1 (F-Score)** = (2*vp) / (2*vp + fp + fn) if (2*vp + fp + fn) > 0 else 0.0 
	A m√©dia harm√¥nica entre Precis√£o e Recall, √∫til para datasets desbalanceados. <br>
 
*Obs: if > 0 else 0.0 evita divis√£o por zero.

Todas as m√©tricas calculadas aqui dependem do threshold espec√≠fico que definimos (>=0.5). <br><br>

**Resultados:**
<p align="center" width="100%">
	<img width="316" height="108" alt="image" src="https://github.com/user-attachments/assets/66f2565f-0756-4701-ab43-8b94260a5e7b" />
</p>

- **Acur√°cia:** Em 92% das imagens o modelo acerta a classe. <br><br>
- **Precis√£o:** Entre todas as imagens que o modelo previu como "c√£o", 91% realmente s√£o c√£es. Se o modelo rotula "c√£o", h√° alta confian√ßa de que √© c√£o. <br><br>
- **Sensibilidade (Recall):** De todos os c√£es do dataset, o modelo identifica 92% como "c√£o". Poucos c√£es s√£o classificados err√¥neamente como gatos. <br><br>
- **Especificidade:** De todos os gatos do dataset, o modelo identifica 92% como "gato". Poucos gatos s√£o classificados err√¥neamente como c√£es. <br><br>
- **F1:** Bom equil√≠brio entre n√£o acusar gato de ser c√£o (FP) e n√£o deixar c√£es passarem despercebidos (FN). 

Por fim, para medir a habilidade do classificador de **distinguir entre as classes** vamos visualizar a curva AUC-ROC (Area Under the Curve - Receiver Operating Curve). Essa m√©trica n√£o depende de um threshold espec√≠fico, ela gera uma vis√£o geral da performance do modelo ao longo de todos os thresholds, sendo muito √∫til para quando temos classes desbalanceadas. As medidas utilizadas para calcular a curva s√£o: TPR (True Positive rate), que √© o mesmo que Recall e FPR, que representa com qual frequ√™ncia o modelo classifica incorretamente inst√¢ncias negativas como positivas. Quanto mais **pr√≥xima de 1.0** a TPR for, e **maior a √°rea sob a curva** mais preciso o modelo. 

**Definindo vari√°veis:**
<p align="center" width="100%">
	<img width="439" height="77" alt="image" src="https://github.com/user-attachments/assets/f5d017cf-2552-4718-a1d0-3a30a2711530" />
</p>

**Gr√°fico:**
<p align="center" width="100%">
	<img width="646" height="470" alt="image" src="https://github.com/user-attachments/assets/8502ff31-e0c7-46ab-b1f6-902a67b9a03d" />
</p>

## Conclus√£o üê±üê∂
Este projeto possibilitou uma compreens√£o mais profunda sobre a **import√¢ncia** das m√©tricas de avalia√ß√£o na an√°lise de modelos de intelig√™ncia artificial. Mais do que apenas treinar uma rede neural, o foco esteve em **interpretar seus resultados** e entender como medidas como acur√°cia, precis√£o, recall, especificidade, F1 e AUC-ROC refletem diferentes aspectos da performance. Essas m√©tricas s√£o fundamentais para diagnosticar erros, comparar modelos e alinhar a escolha do classificador com os **objetivos do problema real**, tornando-se indispens√°veis no processo de desenvolvimento de solu√ß√µes robustas em machine learning.

## Contato üìß
Estou aberto a novas conex√µes e cr√≠ticas construtivas! Voc√™ pode me encontrar em:

Email: lucascondessabertuol@gmail.com

LinkedIn: https://www.linkedin.com/in/lucasbertuol/
